from chroma_utils import query_collections
from openai import OpenAI
from dotenv import load_dotenv
import os

# Load .env file
load_dotenv()

# Initialize OpenAI client
try:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
except Exception as e:
    raise ValueError(f"Failed to initialize OpenAI client: {str(e)}")

class SimpleRAG:
    @staticmethod
    def simple_rag_func(query, collections, top_k=3, fine_tune=False,tempr_val=0,maxtoken=500):
        print(f"Debug simple_rag_func: Query: {query}, Type: {type(query)}")
        """
        Performs Simple RAG by querying ChromaDB collections and optionally fine-tuning the answer using OpenAI.
        Always prints retrieved chunks with similarity scores.
        
        Args:
            query (str): The user's question.
            collections (list): List of ChromaDB collections to query.
            top_k (int): Number of top relevant chunks to retrieve (default: 3).
            fine_tune (bool): If True, pass RAG response to OpenAI for refinement (default: False).
        
        Returns:
            str: The RAG response (raw chunks or fine-tuned answer) with page references.
        """
        try:
            if not collections:
                return "No document collections available to query."
            
            # Query collections to get relevant chunks, metadata, and similarity scores
            results = query_collections(query, collections, top_k)
            if not results:
                return "No relevant documents found for the query."
            
            # Construct raw RAG response and print retrieved chunks
            raw_response = ""
            context = ""
            print("\nRetrieved Chunks:")
            for i, result in enumerate(results):
                doc = result["document"]
                metadata = result["metadata"]
                similarity = result["similarity"]
                page_numbers = metadata.get("page_numbers", [])
                page_ref = f"Pages {', '.join(map(str, page_numbers))}" if page_numbers else "No page info"
                print(f"Chunk {i+1} (Cosine Similarity: {similarity:.3f}, {page_ref}):")
                #print(f"{doc[:200]}..." if len(doc) > 200 else doc)
                print(doc)
                print("-" * 50)
                chunk_text = f"Chunk {i+1} (Similarity: {similarity:.3f}, {page_ref}):\n{doc}\n"
                raw_response += chunk_text
                context += f"Document {i+1} ({page_ref}):\n{doc}\n\n"
            
            # If fine_tune is False, return raw RAG response
            if not fine_tune:
                return f"\nRaw RAG Response:\n{raw_response}"
            
            # If fine_tune is True, pass to OpenAI for refinement
            prompt = f"""Context:
            {context}

            Question: {query}
            Provide a detailed answer based on the context generated by RAG model,
            referencing relevant page numbers if applicable:
            """
            
            # Generate response using OpenAI
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=maxtoken,
                temperature=tempr_val
            )
            
            answer = response.choices[0].message.content.strip()
            return f"\nFine-Tuned RAG Answer:\n{answer}"
        
        except Exception as e:
            return f"Error processing query: {str(e)}"